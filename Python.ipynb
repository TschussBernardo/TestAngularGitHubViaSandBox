{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf6LypbH1LoWfaYtzUn086",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TschussBernardo/TestAngularGitHubViaSandBox/blob/main/Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRXz5CigTzS9",
        "outputId": "39cfdd30-4e9c-4272-f280-1d8933da0c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "erreur\n",
            " : 12.0\n",
            " : 24.0\n",
            " : 36.0\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "import csv\n",
        "\n",
        "with open( \"/content/sample_data/index.html\", 'r') as file_parse:\n",
        "  soup = bs4.BeautifulSoup(file_parse,'html.parser')\n",
        "##consititution des titre pour l'entete du fichier csv\n",
        "titre = [\"Numero du Produit\", \"Prix du Produit\", \"Description du Produit\"]\n",
        "\n",
        "##consitutions des contenus\n",
        "ligne = []\n",
        "liste_numero_h2 = []\n",
        "liste_prix_description = []\n",
        "liste_prix = []\n",
        "liste_description = []\n",
        "\n",
        "##methode de conversion en prix dollar\n",
        "def conversion_prix_en_dolar(prix):\n",
        "  prix =  prix.replace(\"€\", \"$\").replace(\",\", \".\")\n",
        "  ##supprimer les caracteres qui ne sont pas des chiffres\n",
        "  prix = prix.replace(\".\", \"\").replace(\"$\", \"\").replace(\"Prix :\", \"\")\n",
        "  prixdollar = float(prix)*1.2\n",
        "  print(f\" : {prixdollar}\")\n",
        "  return prixdollar\n",
        "\n",
        "\n",
        "tout_numero_h2 = soup.find_all('h2')\n",
        "for numero_h2 in tout_numero_h2:\n",
        "  liste_numero_h2.append(numero_h2.string)\n",
        "\n",
        "tout_dscription = soup.find_all('p')\n",
        "for description in tout_dscription:\n",
        "  if description.string.startswith(\"Prix :\"):\n",
        "    prix_converti_dollar = conversion_prix_en_dolar(description.string)\n",
        "    liste_prix.append(prix_converti_dollar)\n",
        "  elif description.string.startswith(\"Description :\"):\n",
        "    liste_description.append(description.string)\n",
        "  else:\n",
        "    print(\"erreur\")\n",
        "\n",
        "##Création d'fichier CSV pour stocker les données\n",
        "with open(\"data2.csv\", \"w\") as fichier_csv:\n",
        "  writer = csv.writer(fichier_csv, delimiter=',')\n",
        "  writer.writerow(titre)\n",
        "  for numero_h2, prix, description in zip(liste_numero_h2, liste_prix, liste_description):\n",
        "    writer.writerow([numero_h2, prix, description])\n",
        "  ##Récupération des titres et descriptions\n",
        "  ##Parcours des titres et descriptions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "\n",
        "#creation d'un tableau de taille (2,3)\n",
        "tableau = np.array([[1,2,3],[4,5,6]])\n",
        "\n",
        "liste = [5000,1500,2500,3500,1000,2400,1700,1800,9000,4500,7400]\n",
        "\n",
        "revenus = np.array(liste)\n",
        "\n",
        "revenus_sup_3000 = revenus[revenus > 3000]\n",
        "print(revenus_sup_3000)\n",
        "\n",
        "\n",
        "somme_revenus_annuel = revenus.sum()*12\n",
        "print(somme_revenus_annuel)\n",
        "\n",
        "moyenne_revenus = np.mean(revenus)\n",
        "##recupere l'element == 2500 et le retourne\n",
        "revenus[revenus == 2500] = 2500+1441\n",
        "print(revenus)\n",
        "\n",
        "## le plus petit element d'un tableau\n",
        "print(revenus.argmin())\n",
        "\n",
        "#A partir du fichier data2.csv je crée une liste de liste\n",
        "with open(\"data2.csv\", \"r\") as fichier_csv:\n",
        "  reader = csv.reader(fichier_csv, delimiter=',')\n",
        "  liste_ligne = np.array([])\n",
        "  for ligne in reader:\n",
        "    liste_ligne.\n",
        "  liste_ligne.remove(liste_ligne[0])\n",
        "print(liste_ligne)\n",
        "\n",
        "np.array(liste_ligne)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LSrpj_JJVPBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LATbrCS8Xg9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import bs4\n",
        "import csv\n",
        "import subprocess\n",
        "\n",
        "# lien de la page à scrapper\n",
        "url = \"https://www.gov.uk/search/news-and-communications\"\n",
        "reponse = requests.get(url)\n",
        "page = reponse.content\n",
        "\n",
        "# affiche la page HTML et le met dans une page.txt\n",
        "#subprocess.run(f\"echo {page} >> page.txt\", shell=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# transforme (parse) le HTML en objet BeautifulSoup\n",
        "soup = bs4.BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "# récupération de tous les titres\n",
        "titres = soup.find_all(\"a\", class_=\"govuk-footer__link\")\n",
        "\n",
        "titre_textes = []\n",
        "for titre in titres:\n",
        "\ttitre_textes.append(titre.string)\n",
        "\n",
        "# récupération de toutes les descriptions\n",
        "descriptions = soup.find_all(\"p\", class_=\"gem-c-document-list__item-description\")\n",
        "description_textes = []\n",
        "\n",
        "for description in descriptions:\n",
        "\tdescription_textes.append(description.string)\n",
        "\n",
        "# création du fichier data.csv\n",
        "en_tete = ['titre', 'description']\n",
        "with open('data.csv', 'w') as fichier_csv:\n",
        "\twriter = csv.writer(fichier_csv, delimiter=',')\n",
        "\twriter.writerow(en_tete)\n",
        "\t# zip permet d'itérer sur deux listes à la fois\n",
        "\tfor titre, description in zip(titre_textes, description_textes):\n",
        "\t\twriter.writerow([titre, description])\n"
      ],
      "metadata": {
        "id": "cSj4ZDrjW93B"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}